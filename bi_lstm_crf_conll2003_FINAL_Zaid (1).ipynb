{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "bi_lstm_crf_conll2003_FINAL_Zaid.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AtO26COvqJZ",
        "outputId": "41719e56-ddb2-4b33-c5b1-4e8a52f87f75"
      },
      "source": [
        "### THIS IS THE FINAL NOTEBOOK\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = \"/content/drive/Shareddrives/SWM/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyVCOclnqbm9",
        "outputId": "4a30ea3b-a27b-4744-b36c-eacf142ba902"
      },
      "source": [
        "# from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "\n",
        "import time\n",
        "import _pickle as cPickle\n",
        "\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 80\n",
        "plt.style.use('seaborn-pastel')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "print(\"Import Successful.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-8qsrEqbnA",
        "outputId": "353e5ea9-3415-46e8-db8b-288ee3a3abe1"
      },
      "source": [
        "#parameters for the Model\n",
        "def set_params():\n",
        "    parameters = OrderedDict()\n",
        "    parameters['train'] = \"/content/eng.train\" #Path to train file\n",
        "    parameters['dev'] = \"/content/eng.testa\" #Path to test file\n",
        "    parameters['test'] = \"/content/eng.testb\" #Path to dev file\n",
        "    parameters['tag_scheme'] = \"iob\" #BIO or BIOES\n",
        "    parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
        "    parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
        "    parameters['char_dim'] = 30 #Char embedding dimension\n",
        "    parameters['word_dim'] = 50 #Token embedding dimension\n",
        "    parameters['word_lstm_dim'] = 100 #Token LSTM hidden layer size\n",
        "    parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
        "    parameters['embedding_path'] = \"/content/glove.6B.50d.txt\"  #Location of pretrained embeddings\n",
        "    parameters['all_emb'] = 1 #Load all embeddings\n",
        "    parameters['crf'] =1 #Use CRF (0 to disable)\n",
        "    parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
        "    parameters['epoch'] =  25 #Number of epochs to run\"\n",
        "    parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
        "    parameters['name'] = \"self-trained-model\" # Model name\n",
        "    parameters['gradient_clip']=5.0\n",
        "    parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
        "    parameters['reload'] = \"models/pre-trained-model\" \n",
        "    return parameters\n",
        "\n",
        "#Constants\n",
        "parameters = set_params()\n",
        "use_gpu = parameters['use_gpu']\n",
        "START_TAG = '<START>'\n",
        "STOP_TAG = '<STOP>'\n",
        "models_path = \"models/\"\n",
        "mapping_file = '/content/mapping.pkl'\n",
        "name = parameters['name']\n",
        "model_name = models_path + name\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "\n",
        "print(mapping_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mapping.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGLzTJkIqbnC"
      },
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace every digit in a string by a zero.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path, zeros):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in codecs.open(path, 'r', 'utf8'):\n",
        "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
        "        if not line:\n",
        "            if len(sentence) > 0:\n",
        "                if 'DOCSTART' not in sentence[0][0]:\n",
        "                    sentences.append(sentence)\n",
        "                sentence = []\n",
        "        else:\n",
        "            word = line.split()\n",
        "            assert len(word) >= 2\n",
        "            sentence.append(word)\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAE35rJuqbnC"
      },
      "source": [
        "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
        "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
        "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB84XO-dqbnC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "80626826-ae63-4bc5-f7f3-8eaeaa626a1e"
      },
      "source": [
        "def iob2(tags):\n",
        "    \"\"\"\n",
        "    Check that tags have a valid BIO format.\n",
        "    Tags in BIO1 format are converted to BIO2.\n",
        "    \"\"\"\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            continue\n",
        "        split = tag.split('-')\n",
        "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
        "            return False\n",
        "        if split[0] == 'B':\n",
        "            continue\n",
        "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "        elif tags[i - 1][1:] == tag[1:]:\n",
        "            continue\n",
        "        else:  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "    return True\n",
        "\n",
        "def iob_iobes(tags):\n",
        "    \"\"\"\n",
        "    the function is used to convert\n",
        "    BIO -> BIOES tagging\n",
        "    \"\"\"\n",
        "    new_tags = []\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            new_tags.append(tag)\n",
        "        elif tag.split('-')[0] == 'B':\n",
        "            if i + 1 != len(tags) and \\\n",
        "               tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('B-', 'S-'))\n",
        "        elif tag.split('-')[0] == 'I':\n",
        "            if i + 1 < len(tags) and \\\n",
        "                    tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('I-', 'E-'))\n",
        "        else:\n",
        "            raise Exception('Invalid IOB format!')\n",
        "    return new_tags\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "    \"\"\"\n",
        "    Check and update sentences tagging scheme to IOB2.\n",
        "    Only IOB1 and IOB2 schemes are accepted.\n",
        "    \"\"\"\n",
        "    for i, s in enumerate(sentences):\n",
        "        tags = [w[-1] for w in s]\n",
        "        # Check that tags are given in the IOB format\n",
        "        if not iob2(tags):\n",
        "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "            raise Exception('Sentences should be given in IOB format! ' +\n",
        "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
        "        #print(tag_scheme)\n",
        "        if tag_scheme == 'iob':\n",
        "            # If format was IOB1, we convert to IOB2\n",
        "            for word, new_tag in zip(s, tags):\n",
        "                word[-1] = new_tag\n",
        "        elif tag_scheme == 'iobes':\n",
        "            new_tags = iob_iobes(tags)\n",
        "            for word, new_tag in zip(s, new_tags):\n",
        "                word[-1] = new_tag\n",
        "        else:\n",
        "            raise Exception('Unknown tagging scheme!')\n",
        "\n",
        "'''\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "    \"\"\"\n",
        "    Check and update sentences tagging scheme to BIO2\n",
        "    Only BIO1 and BIO2 schemes are accepted for input data.\n",
        "    \"\"\"\n",
        "    for i, s in enumerate(sentences):\n",
        "        tags = [w[-1] for w in s]\n",
        "        # Check that tags are given in the BIO format\n",
        "        if not iob2(tags):\n",
        "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "            raise Exception('Sentences should be given in BIO format! ' +\n",
        "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
        "        if tag_scheme == 'BIOES':\n",
        "            new_tags = iob_iobes(tags)\n",
        "            for word, new_tag in zip(s, new_tags):\n",
        "                word[-1] = new_tag\n",
        "        else:\n",
        "            raise Exception('Wrong tagging scheme!')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef update_tag_scheme(sentences, tag_scheme):\\n    \"\"\"\\n    Check and update sentences tagging scheme to BIO2\\n    Only BIO1 and BIO2 schemes are accepted for input data.\\n    \"\"\"\\n    for i, s in enumerate(sentences):\\n        tags = [w[-1] for w in s]\\n        # Check that tags are given in the BIO format\\n        if not iob2(tags):\\n            s_str = \\'\\n\\'.join(\\' \\'.join(w) for w in s)\\n            raise Exception(\\'Sentences should be given in BIO format! \\' +\\n                            \\'Please check sentence %i:\\n%s\\' % (i, s_str))\\n        if tag_scheme == \\'BIOES\\':\\n            new_tags = iob_iobes(tags)\\n            for word, new_tag in zip(s, new_tags):\\n                word[-1] = new_tag\\n        else:\\n            raise Exception(\\'Wrong tagging scheme!\\')\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98RejQaGqbnD"
      },
      "source": [
        "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSXr_6PmqbnE"
      },
      "source": [
        "def create_dico(item_list):\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list\n",
        "    dico = {}\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico:\n",
        "                dico[item] = 1\n",
        "            else:\n",
        "                dico[item] += 1\n",
        "    return dico\n",
        "\n",
        "def create_mapping(dico):\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences, lower):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
        "    dico = create_dico(words)\n",
        "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words)\n",
        "    ))\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def char_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and mapping of characters, sorted by frequency.\n",
        "    \"\"\"\n",
        "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
        "    dico = create_dico(chars)\n",
        "    char_to_id, id_to_char = create_mapping(dico)\n",
        "    print(\"Found %i unique characters\" % len(dico))\n",
        "    return dico, char_to_id, id_to_char\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    dico[START_TAG] = -1\n",
        "    dico[STOP_TAG] = -2\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "pJhGpthjqbnE",
        "outputId": "54590928-99f8-4835-c17c-86f989115e66"
      },
      "source": [
        "data_dir='/content/'\n",
        "def pickle_me(name, obj):\n",
        "    file_path = data_dir + \"pickle2/\" + name + \".pkl\"\n",
        "    file_obj = open(file_path, 'wb')\n",
        "    cPickle.dump(obj, file_obj)\n",
        "    file_obj.close()\n",
        "    print(\"Pickled the obj {} at {}\".format(name, file_path))\n",
        "    return\n",
        "\n",
        "\n",
        "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
        "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
        "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
        "\n",
        "# pickle_me('word_to_id', word_to_id)\n",
        "# pickle_me('id_to_word', id_to_word)\n",
        "# pickle_me('char_to_id', char_to_id)\n",
        "# pickle_me('id_to_char', id_to_char)\n",
        "# pickle_me('tag_to_id', tag_to_id)\n",
        "# pickle_me('id_to_tag', id_to_tag)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17493 unique words (203621 in total)\n",
            "Found 75 unique characters\n",
            "Found 11 unique named entity tags\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-69c86e5d6bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdico_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_to_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id_to_word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'char_to_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-69c86e5d6bdc>\u001b[0m in \u001b[0;36mpickle_me\u001b[0;34m(name, obj)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"pickle2/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/pickle2/word_to_id.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssodt4mqbnG"
      },
      "source": [
        "def lower_case(x,lower=False):\n",
        "    if lower:\n",
        "        return x.lower()  \n",
        "    else:\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cLmC7-CqbnH",
        "outputId": "9c9772c3-796d-4124-ea32-5612e43861cc"
      },
      "source": [
        "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
        "        - word indexes\n",
        "        - word char indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for s in sentences:\n",
        "        str_words = [w[0] for w in s]\n",
        "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
        "                 for w in str_words]\n",
        "        # Skip characters that are not in the training set\n",
        "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s]\n",
        "        data.append({\n",
        "            'str_words': str_words,\n",
        "            'words': words,\n",
        "            'chars': chars,\n",
        "            'tags': tags,\n",
        "        })\n",
        "    return data\n",
        "\n",
        "train_data = prepare_dataset(\n",
        "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "dev_data = prepare_dataset(\n",
        "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "test_data = prepare_dataset(\n",
        "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14041 / 3250 / 3453 sentences in train / dev / test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5x5fJBrqbnI",
        "outputId": "919bff9f-9729-4746-d174-22f6b616f68f"
      },
      "source": [
        "all_word_embeds = {}\n",
        "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
        "    s = line.strip().split()\n",
        "    if len(s) == parameters['word_dim'] + 1:\n",
        "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
        "\n",
        "#Intializing Word Embedding Matrix\n",
        "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
        "\n",
        "for w in word_to_id:\n",
        "    if w in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
        "    elif w.lower() in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
        "\n",
        "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 pretrained embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdwK6i8gqbnI",
        "outputId": "23f582c0-e987-4f84-fe27-7a1d51fb8733"
      },
      "source": [
        "with open(mapping_file, 'wb') as f:\n",
        "    mappings = {\n",
        "        'word_to_id': word_to_id,\n",
        "        'tag_to_id': tag_to_id,\n",
        "        'char_to_id': char_to_id,\n",
        "        'parameters': parameters,\n",
        "        'word_embeds': word_embeds\n",
        "    }\n",
        "    cPickle.dump(mappings, f)\n",
        "\n",
        "print('word_to_id: ', len(word_to_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_to_id:  17493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-sblEjCqbnJ"
      },
      "source": [
        "def init_embedding(input_embedding):\n",
        "    \"\"\"\n",
        "    Initialize embedding\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "    nn.init.uniform(input_embedding, -bias, bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol46ghHoqbnJ"
      },
      "source": [
        "def init_linear(input_linear):\n",
        "    \"\"\"\n",
        "    Initialize linear transformation\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
        "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
        "    if input_linear.bias is not None:\n",
        "        input_linear.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v24M6A8qbnK"
      },
      "source": [
        "def init_lstm(input_lstm):\n",
        "    \"\"\"\n",
        "    Initialize lstm\n",
        "    \n",
        "    PyTorch weights parameters:\n",
        "    \n",
        "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
        "            `(hidden_size * hidden_size)`\n",
        "            \n",
        "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * hidden_size)`            \n",
        "    \"\"\"\n",
        "    \n",
        "    # Weights init for forward layer\n",
        "    for ind in range(0, input_lstm.num_layers):\n",
        "        \n",
        "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
        "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
        "        \n",
        "        # Initialize the sampling range\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        \n",
        "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        # Similar to above but for the hidden-hidden weights of the current layer\n",
        "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        \n",
        "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
        "    if input_lstm.bidirectional:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "\n",
        "    # Bias initialization steps\n",
        "    \n",
        "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
        "    if input_lstm.bias:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
        "            \n",
        "            # Initializing to zero\n",
        "            bias.data.zero_()\n",
        "            \n",
        "            # This is the range of indices for our forget gates for each LSTM cell\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "            #Similar for the hidden-hidden layer\n",
        "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
        "            bias.data.zero_()\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
        "        if input_lstm.bidirectional:\n",
        "            for ind in range(0, input_lstm.num_layers):\n",
        "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCF96lKqbnL"
      },
      "source": [
        "def log_sum_exp(vec):\n",
        "    '''\n",
        "    This function calculates the score explained above for the forward algorithm\n",
        "    vec 2D: 1 * tagset_size\n",
        "    '''\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "    \n",
        "def argmax(vec):\n",
        "    '''\n",
        "    This function returns the max index in a vector\n",
        "    '''\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return to_scalar(idx)\n",
        "\n",
        "def to_scalar(var):\n",
        "    '''\n",
        "    Function to convert pytorch tensor to a scalar\n",
        "    '''\n",
        "    return var.view(-1).data.tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhnEvW-qbnL"
      },
      "source": [
        "def score_sentences(self, feats, tags):\n",
        "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
        "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
        "    r = torch.LongTensor(range(feats.size()[0]))\n",
        "    if self.use_gpu:\n",
        "        r = r.cuda()\n",
        "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "    else:\n",
        "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "\n",
        "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
        "\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oVtCVprqbnM"
      },
      "source": [
        "def forward_alg(self, feats):\n",
        "    '''\n",
        "    This function performs the forward algorithm explained above\n",
        "    '''\n",
        "    # calculate in log domain\n",
        "    # feats is len(sentence) * tagset_size\n",
        "    # initialize alpha with a Tensor with values all equal to -10000.\n",
        "    \n",
        "    # Do the forward algorithm to compute the partition function\n",
        "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    \n",
        "    # START_TAG has all of the score.\n",
        "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "    \n",
        "    # Wrap in a variable so that we will get automatic backprop\n",
        "    forward_var = autograd.Variable(init_alphas)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "        \n",
        "    # Iterate through the sentence\n",
        "    for feat in feats:\n",
        "        # broadcast the emission score: it is the same regardless of\n",
        "        # the previous tag\n",
        "        emit_score = feat.view(-1, 1)\n",
        "        \n",
        "        # the ith entry of trans_score is the score of transitioning to\n",
        "        # next_tag from i\n",
        "        tag_var = forward_var + self.transitions + emit_score\n",
        "        \n",
        "        # The ith entry of next_tag_var is the value for the\n",
        "        # edge (i -> next_tag) before we do log-sum-exp\n",
        "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
        "        \n",
        "        # The forward variable for this tag is log-sum-exp of all the\n",
        "        # scores.\n",
        "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
        "        \n",
        "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
        "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    # Z(x)\n",
        "    return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkMk4bOdqbnN"
      },
      "source": [
        "def viterbi_algo(self, feats):\n",
        "    '''\n",
        "    In this function, we implement the viterbi algorithm explained above.\n",
        "    A Dynamic programming based approach to find the best tag sequence\n",
        "    '''\n",
        "    backpointers = []\n",
        "    # analogous to forward\n",
        "    \n",
        "    # Initialize the viterbi variables in log space\n",
        "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "    \n",
        "    # forward_var at step i holds the viterbi variables for step i-1\n",
        "    forward_var = Variable(init_vvars)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "    for feat in feats:\n",
        "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
        "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
        "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
        "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
        "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
        "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
        "        if self.use_gpu:\n",
        "            viterbivars_t = viterbivars_t.cuda()\n",
        "            \n",
        "        # Now add in the emission scores, and assign forward_var to the set\n",
        "        # of viterbi variables we just computed\n",
        "        forward_var = viterbivars_t + feat\n",
        "        backpointers.append(bptrs_t)\n",
        "\n",
        "    # Transition to STOP_TAG\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
        "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
        "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
        "    path_score = terminal_var[best_tag_id]\n",
        "    \n",
        "    # Follow the back pointers to decode the best path.\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "        best_tag_id = bptrs_t[best_tag_id]\n",
        "        best_path.append(best_tag_id)\n",
        "        \n",
        "    # Pop off the start tag (we dont want to return that to the caller)\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXUDhkJIqbnN"
      },
      "source": [
        "def forward_calc(self, sentence, chars, chars2_length, d):\n",
        "    \n",
        "    '''\n",
        "    The function calls viterbi decode and generates the \n",
        "    most probable sequence of tags for the sentence\n",
        "    '''\n",
        "    \n",
        "    # Get the emission scores from the BiLSTM\n",
        "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
        "    # viterbi to get tag_seq\n",
        "    \n",
        "    # Find the best path, given the features.\n",
        "    if self.use_crf:\n",
        "        score, tag_seq = self.viterbi_decode(feats)\n",
        "    else:\n",
        "        score, tag_seq = torch.max(feats, 1)\n",
        "        tag_seq = list(tag_seq.cpu().data)\n",
        "\n",
        "    return score, tag_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMrVeSW8qbnO"
      },
      "source": [
        "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
        "    \n",
        "    chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
        "    \n",
        "    ## Creating Character level representation using Convolutional Neural Netowrk\n",
        "    ## followed by a Maxpooling Layer\n",
        "    chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
        "    chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
        "                                         kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
        "\n",
        "    ## Loading word embeddings\n",
        "    embeds = self.word_embeds(sentence)\n",
        "    \n",
        "    ## We concatenate the word embeddings and the character level representation\n",
        "    ## to create unified representation for each word\n",
        "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
        "\n",
        "    embeds = embeds.unsqueeze(1)\n",
        "    \n",
        "    ## Dropout on the unified embeddings\n",
        "    embeds = self.dropout(embeds)\n",
        "    \n",
        "    ## Word lstm\n",
        "    ## Takes words as input and generates a output at each step\n",
        "    lstm_out, _ = self.lstm(embeds)\n",
        "    \n",
        "    ## Reshaping the outputs from the lstm layer\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
        "    \n",
        "    ## Dropout on the lstm output\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "    \n",
        "    ## Linear layer converts the ouput vectors to tag space\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    \n",
        "    return lstm_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btuqgfnRqbnP"
      },
      "source": [
        "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
        "    # sentence, tags is a list of ints\n",
        "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
        "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
        "\n",
        "    if self.use_crf:\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "    else:\n",
        "        tags = Variable(tags)\n",
        "        scores = nn.functional.cross_entropy(feats, tags)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz8HPiL9qbnP"
      },
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
        "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
        "                 , use_crf=True):\n",
        "        '''\n",
        "        Input parameters:\n",
        "                \n",
        "                vocab_size= Size of vocabulary (int)\n",
        "                tag_to_ix = Dictionary that maps NER tags to indices\n",
        "                embedding_dim = Dimension of word embeddings (int)\n",
        "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
        "                char_to_ix = Dictionary that maps characters to indices\n",
        "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
        "                char_out_dimension = Output dimension from the CNN encoder for character\n",
        "                char_embedding_dim = Dimension of the character embeddings\n",
        "                use_gpu = defines availability of GPU, \n",
        "                    when True: CUDA function calls are made\n",
        "                    else: Normal CPU function calls are made\n",
        "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
        "        '''\n",
        "        \n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        \n",
        "        #parameter initialization for the model\n",
        "        self.use_gpu = use_gpu\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.use_crf = use_crf\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.out_channels = char_out_dimension\n",
        "\n",
        "        if char_embedding_dim is not None:\n",
        "            self.char_embedding_dim = char_embedding_dim\n",
        "            \n",
        "            #Initializing the character embedding layer\n",
        "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
        "            init_embedding(self.char_embeds.weight)\n",
        "            \n",
        "            #Performing CNN encoding on the character embeddings\n",
        "            self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
        "\n",
        "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if pre_word_embeds is not None:\n",
        "            #Initializes the word embeddings with pretrained word embeddings\n",
        "            self.pre_word_embeds = True\n",
        "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
        "        else:\n",
        "            self.pre_word_embeds = False\n",
        "    \n",
        "        #Initializing the dropout layer, with dropout specificed in parameters\n",
        "        self.dropout = nn.Dropout(parameters['dropout'])\n",
        "        \n",
        "        #Lstm Layer:\n",
        "        #input dimension: word embedding dimension + character level representation\n",
        "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
        "        \n",
        "        #Initializing the lstm layer using predefined function for initialization\n",
        "        init_lstm(self.lstm)\n",
        "        \n",
        "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "        \n",
        "        #Initializing the linear layer using predefined function for initialization\n",
        "        init_linear(self.hidden2tag) \n",
        "\n",
        "        if self.use_crf:\n",
        "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
        "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
        "            self.transitions = nn.Parameter(\n",
        "                torch.zeros(self.tagset_size, self.tagset_size))\n",
        "            \n",
        "            # These two statements enforce the constraint that we never transfer\n",
        "            # to the start tag and we never transfer from the stop tag\n",
        "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "    #assigning the functions, which we have defined earlier\n",
        "    _score_sentence = score_sentences\n",
        "    _get_lstm_features = get_lstm_features\n",
        "    _forward_alg = forward_alg\n",
        "    viterbi_decode = viterbi_algo\n",
        "    neg_log_likelihood = get_neg_log_likelihood\n",
        "    forward = forward_calc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-bnHns6qbnP",
        "outputId": "3ed7b916-0743-4ba2-ba7c-bb09e48c6c7c"
      },
      "source": [
        "#creating the model using the Class defined above\n",
        "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
        "                   tag_to_ix=tag_to_id,\n",
        "                   embedding_dim=parameters['word_dim'],\n",
        "                   hidden_dim=parameters['word_lstm_dim'],\n",
        "                   use_gpu=use_gpu,\n",
        "                   char_to_ix=char_to_id,\n",
        "                   pre_word_embeds=word_embeds,\n",
        "                   use_crf=parameters['crf'])\n",
        "print(\"Model Initialized!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Initialized!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-IOpT1UqbnP"
      },
      "source": [
        "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
        "if parameters['reload']:\n",
        "    if not os.path.exists(parameters['reload']):\n",
        "      parameters['reload'] = False\n",
        "        # print(\"downloading pre-trained model\")\n",
        "        # model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
        "        # urllib.request.urlretrieve(model_url, parameters['reload'])\n",
        "    else:\n",
        "      model.load_state_dict(torch.load(parameters['reload']))\n",
        "    print(\"model reloaded :\", parameters['reload'])\n",
        "\n",
        "if use_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFHXlezoqbnQ"
      },
      "source": [
        "#Initializing the optimizer\n",
        "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
        "#learning rate=0.015 and momentum=0.9 \n",
        "#decay_rate=0.05 \n",
        "\n",
        "learning_rate = 0.015\n",
        "momentum = 0.9\n",
        "number_of_epochs = parameters['epoch'] \n",
        "decay_rate = 0.05\n",
        "gradient_clip = parameters['gradient_clip']\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "#variables which will used in training process\n",
        "losses = [] #list to store all losses\n",
        "loss = 0.0 #Loss Initializatoin\n",
        "best_dev_F = -1.0 # Current best F-1 Score on Dev Set\n",
        "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
        "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
        "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
        "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
        "plot_every = 2000 # Store loss after this many iterations\n",
        "count = 0 #Counts the number of iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK6PIVHvqbnR"
      },
      "source": [
        "def get_chunk_type(tok, idx_to_tag):\n",
        "    \"\"\"\n",
        "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
        "    as defined in BIOES\n",
        "    \n",
        "    Args:\n",
        "        tok: id of token, ex 4\n",
        "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
        "\n",
        "    Returns:\n",
        "        tuple: \"B\", \"PER\"\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    tag_name = idx_to_tag[tok]\n",
        "    tag_class = tag_name.split('-')[0]\n",
        "    tag_type = tag_name.split('-')[-1]\n",
        "    return tag_class, tag_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kZ9D1BCqbnR"
      },
      "source": [
        "def get_chunks(seq, tags):\n",
        "    \"\"\"Given a sequence of tags, group entities and their position\n",
        "\n",
        "    Args:\n",
        "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
        "        tags: dict[\"O\"] = 4\n",
        "\n",
        "    Returns:\n",
        "        list of (chunk_type, chunk_start, chunk_end)\n",
        "\n",
        "    Example:\n",
        "        seq = [4, 5, 0, 3]\n",
        "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
        "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We assume by default the tags lie outside a named entity\n",
        "    default = tags[\"O\"]\n",
        "    \n",
        "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "    \n",
        "    chunks = []\n",
        "    \n",
        "    chunk_type, chunk_start = None, None\n",
        "    for i, tok in enumerate(seq):\n",
        "        # End of a chunk 1\n",
        "        if tok == default and chunk_type is not None:\n",
        "            # Add a chunk.\n",
        "            chunk = (chunk_type, chunk_start, i)\n",
        "            chunks.append(chunk)\n",
        "            chunk_type, chunk_start = None, None\n",
        "\n",
        "        # End of a chunk + start of a chunk!\n",
        "        elif tok != default:\n",
        "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
        "            if chunk_type is None:\n",
        "                # Initialize chunk for each entity\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
        "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
        "                # or, if the chunk type is different from the previous one, then we\n",
        "                # start labelling it as a new entity\n",
        "                chunk = (chunk_type, chunk_start, i)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # end condition\n",
        "    if chunk_type is not None:\n",
        "        chunk = (chunk_type, chunk_start, len(seq))\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFwmRKCGqbnR"
      },
      "source": [
        "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
        "    '''\n",
        "    The function takes as input the model, data and calcuates F-1 Score\n",
        "    It performs conditional updates \n",
        "     1) Flag to save the model \n",
        "     2) Best F-1 score\n",
        "    ,if the F-1 score calculated improves on the previous F-1 score\n",
        "    '''\n",
        "    # Initializations\n",
        "    prediction = [] # A list that stores predicted tags\n",
        "    save = False # Flag that tells us if the model needs to be saved\n",
        "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
        "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
        "    \n",
        "    for data in datas:\n",
        "        ground_truth_id = data['tags']\n",
        "        words = data['str_words']\n",
        "        chars2 = data['chars']\n",
        "\n",
        "        d = {} \n",
        "        \n",
        "        # Padding the each word to max word size of that sentence\n",
        "        chars2_length = [len(c) for c in chars2]\n",
        "        char_maxl = max(chars2_length)\n",
        "        chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "        for i, c in enumerate(chars2):\n",
        "            chars2_mask[i, :chars2_length[i]] = c\n",
        "        chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "        dwords = Variable(torch.LongTensor(data['words']))\n",
        "        \n",
        "        # We are getting the predicted output from our model\n",
        "        if use_gpu:\n",
        "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "        else:\n",
        "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
        "        predicted_id = out\n",
        "    \n",
        "        \n",
        "        # We use the get chunks function defined above to get the true chunks\n",
        "        # and the predicted chunks from true labels and predicted labels respectively\n",
        "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
        "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
        "                                         tag_to_id))\n",
        "\n",
        "        # Updating the count variables\n",
        "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
        "        total_preds   += len(lab_pred_chunks)\n",
        "        total_correct += len(lab_chunks)\n",
        "    \n",
        "    # Calculating the F1-Score\n",
        "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
        "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
        "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
        "\n",
        "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
        "    \n",
        "    # If our current F1-Score is better than the previous best, we update the best\n",
        "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
        "    \n",
        "    if new_F>best_F:\n",
        "        best_F=new_F\n",
        "        save=True\n",
        "\n",
        "    return best_F, new_F, save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgkP2hdvqbnS"
      },
      "source": [
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"\n",
        "    shrink learning rate\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HaXyqzUKqbnT",
        "outputId": "fca6dbbc-692f-467b-ab6b-9dd52c9cbd10"
      },
      "source": [
        "parameters['reload']=False\n",
        "\n",
        "if not parameters['reload']:\n",
        "    tr = time.time()\n",
        "    model.train(True)\n",
        "    for epoch in range(1,number_of_epochs):\n",
        "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
        "            count += 1\n",
        "            data = train_data[index]\n",
        "\n",
        "            ##gradient updates for each data entry\n",
        "            model.zero_grad()\n",
        "\n",
        "            sentence_in = data['words']\n",
        "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
        "            tags = data['tags']\n",
        "            chars2 = data['chars']\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            ## Padding the each word to max word size of that sentence\n",
        "            chars2_length = [len(c) for c in chars2]\n",
        "            char_maxl = max(chars2_length)\n",
        "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "            for i, c in enumerate(chars2):\n",
        "                chars2_mask[i, :chars2_length[i]] = c\n",
        "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "\n",
        "            targets = torch.LongTensor(tags)\n",
        "\n",
        "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
        "            if use_gpu:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "            else:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
        "            loss += neg_log_likelihood.data / len(data['words'])\n",
        "            # loss += neg_log_likelihood.data[0] / len(data['words'])\n",
        "            neg_log_likelihood.backward()\n",
        "\n",
        "            #we use gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            #Storing loss\n",
        "            if count % plot_every == 0:\n",
        "                loss /= plot_every\n",
        "                # print(count, ': ', loss,'.\\r'))\n",
        "                # print(\"{}:{}\\r\".format(count, loss))\n",
        "                res_info = \"Count: {}, Loss: {}\".format(count, loss)\n",
        "                print(res_info)\n",
        "                # sys.stdout.write(res_info)\n",
        "                # sys.stdout.flush()\n",
        "                if losses == []:\n",
        "                    losses.append(loss)\n",
        "                losses.append(loss)\n",
        "                loss = 0.0\n",
        "\n",
        "            #Evaluating on Train, Test, Dev Sets\n",
        "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
        "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
        "                model.train(False)\n",
        "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
        "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
        "                if save:\n",
        "                    print(\"Saving Model to \", model_name)\n",
        "                    torch.save(model.state_dict(), model_name)\n",
        "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
        "\n",
        "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
        "                print()\n",
        "                model.train(True)\n",
        "\n",
        "            #Performing decay on the learning rate\n",
        "            if count % len(train_data) == 0:\n",
        "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
        "\n",
        "    print(time.time() - tr)\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "if not parameters['reload']:\n",
        "    #reload the best model saved from training\n",
        "    model.load_state_dict(torch.load(model_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count: 2000, Loss: 0.4002072215080261\n",
            "Count: 4000, Loss: 0.2318676859140396\n",
            "Count: 6000, Loss: 0.18124672770500183\n",
            "Count: 8000, Loss: 0.17670582234859467\n",
            "Count: 10000, Loss: 0.1859508603811264\n",
            "Count: 12000, Loss: 0.16694223880767822\n",
            "Count: 14000, Loss: 0.14405809342861176\n",
            "Count: 16000, Loss: 0.14009617269039154\n",
            "Count: 18000, Loss: 0.13447362184524536\n",
            "Count: 20000, Loss: 0.13106466829776764\n",
            "Count: 22000, Loss: 0.13995881378650665\n",
            "Count: 24000, Loss: 0.12850497663021088\n",
            "Count: 26000, Loss: 0.12626998126506805\n",
            "Count: 28000, Loss: 0.12469486892223358\n",
            "Count: 30000, Loss: 0.11755094677209854\n",
            "Count: 32000, Loss: 0.11383330821990967\n",
            "Count: 34000, Loss: 0.1133238896727562\n",
            "Count: 36000, Loss: 0.12197066843509674\n",
            "Count: 38000, Loss: 0.10442732274532318\n",
            "Count: 40000, Loss: 0.1000128984451294\n",
            "Count: 42000, Loss: 0.10663849115371704\n",
            "Count: 44000, Loss: 0.09760741889476776\n",
            "Count: 46000, Loss: 0.09629553556442261\n",
            "Count: 48000, Loss: 0.09912913292646408\n",
            "Count: 50000, Loss: 0.1006103977560997\n",
            "Count: 52000, Loss: 0.08286048471927643\n",
            "Count: 54000, Loss: 0.11111179739236832\n",
            "Count: 56000, Loss: 0.09309563040733337\n",
            "Train: new_F: 0.9293970435409642 best_F: -1.0 \n",
            "Dev: new_F: 0.8712921490746218 best_F: -1.0 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8285561687733666 best_F: -1.0 \n",
            "\n",
            "Count: 58000, Loss: 0.09989841282367706\n",
            "Count: 60000, Loss: 0.08097165822982788\n",
            "Count: 62000, Loss: 0.09530043601989746\n",
            "Count: 64000, Loss: 0.07760682702064514\n",
            "Count: 66000, Loss: 0.09296689182519913\n",
            "Count: 68000, Loss: 0.08402392268180847\n",
            "Count: 70000, Loss: 0.08025816082954407\n",
            "Count: 72000, Loss: 0.07977090030908585\n",
            "Count: 74000, Loss: 0.08395083248615265\n",
            "Count: 76000, Loss: 0.08193577826023102\n",
            "Count: 78000, Loss: 0.08824414014816284\n",
            "Count: 80000, Loss: 0.06583817303180695\n",
            "Count: 82000, Loss: 0.07386267185211182\n",
            "Count: 84000, Loss: 0.07150909304618835\n",
            "Count: 86000, Loss: 0.07941752672195435\n",
            "Count: 88000, Loss: 0.06933014839887619\n",
            "Count: 90000, Loss: 0.06089440733194351\n",
            "Count: 92000, Loss: 0.06944985687732697\n",
            "Count: 94000, Loss: 0.07988482713699341\n",
            "Count: 96000, Loss: 0.0771995484828949\n",
            "Count: 98000, Loss: 0.06765604019165039\n",
            "Count: 100000, Loss: 0.06613249331712723\n",
            "Count: 102000, Loss: 0.06977298110723495\n",
            "Count: 104000, Loss: 0.0648658275604248\n",
            "Count: 106000, Loss: 0.06484709680080414\n",
            "Count: 108000, Loss: 0.05838010832667351\n",
            "Count: 110000, Loss: 0.06499762833118439\n",
            "Count: 112000, Loss: 0.049140140414237976\n",
            "Train: new_F: 0.9562065583557203 best_F: 0.9293970435409642 \n",
            "Dev: new_F: 0.9010504913588614 best_F: 0.8712921490746218 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8527492177022798 best_F: 0.8285561687733666 \n",
            "\n",
            "Count: 114000, Loss: 0.06405150145292282\n",
            "Count: 116000, Loss: 0.06301786005496979\n",
            "Count: 118000, Loss: 0.049693506211042404\n",
            "Count: 120000, Loss: 0.06185794249176979\n",
            "Count: 122000, Loss: 0.06519227474927902\n",
            "Count: 124000, Loss: 0.056923940777778625\n",
            "Count: 126000, Loss: 0.053054019808769226\n",
            "Count: 128000, Loss: 0.05447210371494293\n",
            "Count: 130000, Loss: 0.055420536547899246\n",
            "Count: 132000, Loss: 0.053081922233104706\n",
            "Count: 134000, Loss: 0.059830281883478165\n",
            "Count: 136000, Loss: 0.05776447430253029\n",
            "Count: 138000, Loss: 0.05988575518131256\n",
            "Count: 140000, Loss: 0.05285137891769409\n",
            "Count: 142000, Loss: 0.05981745943427086\n",
            "Count: 144000, Loss: 0.054094113409519196\n",
            "Count: 146000, Loss: 0.045426853001117706\n",
            "Count: 148000, Loss: 0.05024801194667816\n",
            "Count: 150000, Loss: 0.06232403218746185\n",
            "Count: 152000, Loss: 0.05310894176363945\n",
            "Count: 154000, Loss: 0.04956572502851486\n",
            "Count: 156000, Loss: 0.06312724947929382\n",
            "Count: 158000, Loss: 0.05116859823465347\n",
            "Count: 160000, Loss: 0.04108377546072006\n",
            "Count: 162000, Loss: 0.04190409556031227\n",
            "Count: 164000, Loss: 0.04221820831298828\n",
            "Count: 166000, Loss: 0.0522647425532341\n",
            "Count: 168000, Loss: 0.050287820398807526\n",
            "Train: new_F: 0.9694129675385532 best_F: 0.9562065583557203 \n",
            "Dev: new_F: 0.9096145700974164 best_F: 0.9010504913588614 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.860720994722247 best_F: 0.8527492177022798 \n",
            "\n",
            "Count: 170000, Loss: 0.0458279587328434\n",
            "Count: 172000, Loss: 0.04791100323200226\n",
            "Count: 174000, Loss: 0.04517923668026924\n",
            "Count: 176000, Loss: 0.04814701899886131\n",
            "Count: 178000, Loss: 0.048849720507860184\n",
            "Count: 180000, Loss: 0.04636365547776222\n",
            "Count: 182000, Loss: 0.04820458963513374\n",
            "Count: 184000, Loss: 0.04647446051239967\n",
            "Count: 186000, Loss: 0.0465221032500267\n",
            "Count: 188000, Loss: 0.05323243513703346\n",
            "Count: 190000, Loss: 0.04569284990429878\n",
            "Count: 192000, Loss: 0.04415338858962059\n",
            "Count: 194000, Loss: 0.047108061611652374\n",
            "Count: 196000, Loss: 0.03666149824857712\n",
            "Count: 198000, Loss: 0.04195487126708031\n",
            "Count: 200000, Loss: 0.03833999112248421\n",
            "Count: 202000, Loss: 0.042238932102918625\n",
            "Count: 204000, Loss: 0.041058480739593506\n",
            "Count: 206000, Loss: 0.045777689665555954\n",
            "Count: 208000, Loss: 0.044009871780872345\n",
            "Count: 210000, Loss: 0.03272445499897003\n",
            "Count: 212000, Loss: 0.033049076795578\n",
            "Count: 214000, Loss: 0.03718613460659981\n",
            "Count: 216000, Loss: 0.04024428874254227\n",
            "Count: 218000, Loss: 0.03812920302152634\n",
            "Count: 220000, Loss: 0.0418357253074646\n",
            "Count: 222000, Loss: 0.04125704988837242\n",
            "Count: 224000, Loss: 0.043402884155511856\n",
            "Train: new_F: 0.9763990474570505 best_F: 0.9694129675385532 \n",
            "Dev: new_F: 0.9066013016651171 best_F: 0.9096145700974164 \n",
            "Test: new_F: 0.8523801034052416 best_F: 0.860720994722247 \n",
            "\n",
            "Count: 226000, Loss: 0.040777478367090225\n",
            "Count: 228000, Loss: 0.03632856160402298\n",
            "Count: 230000, Loss: 0.035371407866477966\n",
            "Count: 232000, Loss: 0.04884845018386841\n",
            "Count: 234000, Loss: 0.034288372844457626\n",
            "Count: 236000, Loss: 0.041043464094400406\n",
            "Count: 238000, Loss: 0.034031905233860016\n",
            "Count: 240000, Loss: 0.04246342554688454\n",
            "Count: 242000, Loss: 0.035041794180870056\n",
            "Count: 244000, Loss: 0.032813843339681625\n",
            "Count: 246000, Loss: 0.039744019508361816\n",
            "Count: 248000, Loss: 0.031466737389564514\n",
            "Count: 250000, Loss: 0.03527660295367241\n",
            "Count: 252000, Loss: 0.03603927791118622\n",
            "Count: 254000, Loss: 0.044598229229450226\n",
            "Count: 256000, Loss: 0.0389186330139637\n",
            "Count: 258000, Loss: 0.029913857579231262\n",
            "Count: 260000, Loss: 0.044404588639736176\n",
            "Count: 262000, Loss: 0.03741643205285072\n",
            "Count: 264000, Loss: 0.03139636293053627\n",
            "Count: 266000, Loss: 0.03987612575292587\n",
            "Count: 268000, Loss: 0.033993370831012726\n",
            "Count: 270000, Loss: 0.03161247447133064\n",
            "Count: 272000, Loss: 0.03300698101520538\n",
            "Count: 274000, Loss: 0.035098541527986526\n",
            "Count: 276000, Loss: 0.02859814465045929\n",
            "Count: 278000, Loss: 0.03808172792196274\n",
            "Count: 280000, Loss: 0.03865620493888855\n",
            "Count: 282000, Loss: 0.029824167490005493\n",
            "Count: 284000, Loss: 0.032834261655807495\n",
            "Count: 286000, Loss: 0.027430091053247452\n",
            "Count: 288000, Loss: 0.02881331369280815\n",
            "Count: 290000, Loss: 0.03820657357573509\n",
            "Count: 292000, Loss: 0.03644244372844696\n",
            "Count: 294000, Loss: 0.03734645992517471\n",
            "Train: new_F: 0.9793529161345254 best_F: 0.9763990474570505 \n",
            "Dev: new_F: 0.9118272255760564 best_F: 0.9096145700974164 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8664987405541561 best_F: 0.860720994722247 \n",
            "\n",
            "Count: 296000, Loss: 0.034730810672044754\n",
            "Count: 298000, Loss: 0.025835411623120308\n",
            "Count: 300000, Loss: 0.031787365674972534\n",
            "Count: 302000, Loss: 0.0345725417137146\n",
            "Count: 304000, Loss: 0.03714563325047493\n",
            "Count: 306000, Loss: 0.03137651085853577\n",
            "Count: 308000, Loss: 0.03244083374738693\n",
            "Train: new_F: 0.9819025719638902 best_F: 0.9793529161345254 \n",
            "Dev: new_F: 0.9116421464410239 best_F: 0.9118272255760564 \n",
            "Test: new_F: 0.8637305232819958 best_F: 0.8664987405541561 \n",
            "\n",
            "Count: 310000, Loss: 0.028838302940130234\n",
            "Count: 312000, Loss: 0.034311093389987946\n",
            "Count: 314000, Loss: 0.03290466219186783\n",
            "Count: 316000, Loss: 0.0335187166929245\n",
            "Count: 318000, Loss: 0.02850789949297905\n",
            "Count: 320000, Loss: 0.02957770600914955\n",
            "Count: 322000, Loss: 0.02629058249294758\n",
            "Train: new_F: 0.9806800289374016 best_F: 0.9819025719638902 \n",
            "Dev: new_F: 0.9148467919417639 best_F: 0.9118272255760564 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8676141950478232 best_F: 0.8664987405541561 \n",
            "\n",
            "Count: 324000, Loss: 0.028681078925728798\n",
            "Count: 326000, Loss: 0.03511203080415726\n",
            "Count: 328000, Loss: 0.03163396567106247\n",
            "Count: 330000, Loss: 0.02710697054862976\n",
            "Count: 332000, Loss: 0.027678249403834343\n",
            "Count: 334000, Loss: 0.02967832423746586\n",
            "Count: 336000, Loss: 0.02582080289721489\n",
            "Train: new_F: 0.9815551537070524 best_F: 0.9819025719638902 \n",
            "Dev: new_F: 0.9136599032175906 best_F: 0.9148467919417639 \n",
            "Test: new_F: 0.8640402046127614 best_F: 0.8676141950478232 \n",
            "\n",
            "5427.783722639084\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'values'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-d5a5fff1bab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x320 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mgWXgMxDLZ5"
      },
      "source": [
        "#pickle_me('model', model)\n",
        "\n",
        "import pickle \n",
        "dbfile = open('X_train', 'ab')\n",
        "  \n",
        "# source, destination\n",
        "pickle.dump(model, dbfile)                     \n",
        "dbfile.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNP206ucqbnU",
        "outputId": "bf8b89f5-90a7-4945-9b8f-a77a64d7ee0f"
      },
      "source": [
        "model_testing_sentences = [\"I am at Tempe\", \"Germany representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.\"]\n",
        "\n",
        "#parameters\n",
        "lower=parameters['lower']\n",
        "\n",
        "#preprocessing\n",
        "final_test_data = []\n",
        "for sentence in model_testing_sentences:\n",
        "    s=sentence.split()\n",
        "    str_words = [w for w in s]\n",
        "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
        "    \n",
        "    # Skip characters that are not in the training set\n",
        "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
        "    \n",
        "    final_test_data.append({\n",
        "        'str_words': str_words,\n",
        "        'words': words,\n",
        "        'chars': chars,\n",
        "    })\n",
        "\n",
        "#prediction\n",
        "predictions = []\n",
        "print(\"Prediction:\")\n",
        "print(\"word : tag\")\n",
        "for data in final_test_data:\n",
        "    words = data['str_words']\n",
        "    chars2 = data['chars']\n",
        "\n",
        "    d = {} \n",
        "    \n",
        "    # Padding the each word to max word size of that sentence\n",
        "    chars2_length = [len(c) for c in chars2]\n",
        "    char_maxl = max(chars2_length)\n",
        "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "    for i, c in enumerate(chars2):\n",
        "        chars2_mask[i, :chars2_length[i]] = c\n",
        "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "    dwords = Variable(torch.LongTensor(data['words']))\n",
        "\n",
        "    # We are getting the predicted output from our model\n",
        "    if use_gpu:\n",
        "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "    else:\n",
        "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
        "\n",
        "    pred_chunks = get_chunks(predicted_id,tag_to_id)\n",
        "    temp_list_tags=['NA']*len(words)\n",
        "    for p in pred_chunks:\n",
        "        temp_list_tags[p[1]]=p[0]\n",
        "        \n",
        "    for word,tag in zip(words,temp_list_tags):\n",
        "        print(word,':',tag)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\n",
            "word : tag\n",
            "When : NA\n",
            "Sebastian : NA\n",
            "Thrun : NA\n",
            "started : NA\n",
            "working : NA\n",
            "on : NA\n",
            "self-driving : NA\n",
            "cars : NA\n",
            "at : NA\n",
            "Google : LOC\n",
            "in : NA\n",
            "2007, : NA\n",
            "few : NA\n",
            "people : NA\n",
            "outside : NA\n",
            "of : NA\n",
            "the : NA\n",
            "company : NA\n",
            "took : NA\n",
            "him : NA\n",
            "seriously. : NA\n",
            "I : PER\n",
            "can : NA\n",
            "tell : NA\n",
            "you : NA\n",
            "very : NA\n",
            "senior : NA\n",
            "CEOs : NA\n",
            "of : NA\n",
            "major : NA\n",
            "American : MISC\n",
            "car : NA\n",
            "companies : NA\n",
            "would : NA\n",
            "shake : NA\n",
            "my : NA\n",
            "hand : NA\n",
            "and : NA\n",
            "turn : NA\n",
            "away : NA\n",
            "because : NA\n",
            "I : NA\n",
            "wasnt : NA\n",
            "worth : NA\n",
            "talking : NA\n",
            "to, : NA\n",
            "said : NA\n",
            "Thrun, : PER\n",
            "now : NA\n",
            "the : NA\n",
            "co-founder : NA\n",
            "and : NA\n",
            "CEO : NA\n",
            "of : NA\n",
            "online : NA\n",
            "higher : NA\n",
            "education : NA\n",
            "startup : NA\n",
            "Udacity, : ORG\n",
            "in : NA\n",
            "an : NA\n",
            "interview : NA\n",
            "with : NA\n",
            "Recode : NA\n",
            "earlier : NA\n",
            "this : NA\n",
            "week.A : NA\n",
            "little : NA\n",
            "less : NA\n",
            "than : NA\n",
            "a : NA\n",
            "decade : NA\n",
            "later, : NA\n",
            "dozens : NA\n",
            "of : NA\n",
            "self-driving : NA\n",
            "startups : NA\n",
            "have : NA\n",
            "cropped : NA\n",
            "up : NA\n",
            "while : NA\n",
            "automakers : NA\n",
            "around : NA\n",
            "the : NA\n",
            "world : NA\n",
            "clamor, : NA\n",
            "wallet : NA\n",
            "in : NA\n",
            "hand, : NA\n",
            "to : NA\n",
            "secure : NA\n",
            "their : NA\n",
            "place : NA\n",
            "in : NA\n",
            "the : NA\n",
            "fast-moving : NA\n",
            "world : NA\n",
            "of : NA\n",
            "fully : NA\n",
            "automated : NA\n",
            "transportation. : NA\n",
            "\n",
            "\n",
            "Germany : LOC\n",
            "representative : NA\n",
            "to : NA\n",
            "the : NA\n",
            "European : ORG\n",
            "Union's : NA\n",
            "veterinary : NA\n",
            "committee : NA\n",
            "Werner : PER\n",
            "Zwingmann : NA\n",
            "said : NA\n",
            "on : NA\n",
            "Wednesday : NA\n",
            "consumers : NA\n",
            "should : NA\n",
            "buy : NA\n",
            "sheepmeat : NA\n",
            "from : NA\n",
            "countries : NA\n",
            "other : NA\n",
            "than : NA\n",
            "Britain : LOC\n",
            "until : NA\n",
            "the : NA\n",
            "scientific : NA\n",
            "advice : NA\n",
            "was : NA\n",
            "clearer. : NA\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esf_1CJUymJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}